---
title:  Brief survey on Reinforcement Learning
date: 2025-03-29 14:00:00 +0800
categories: [Survey, RL]
tags: [reinforcement learning]     # TAG names should always be lowercase
---

## 汇报内容：各种强化学习方法对比

科研中常见的强化学习算法主要可以分为以下几类：

1. **基于值函数的方法**
   例如传统的 Q-learning 和 SARSA，以及它们的深度版本——深度 Q 网络（DQN）。在 DQN 基础上，还有 Double DQN、Dueling DQN 和 Prioritized Experience Replay 等变体，这类方法主要适用于离散动作空间，通过迭代更新 Q 值来估计状态-动作对的长期回报。
2. **基于策略梯度的方法**
   直接通过梯度上升来优化策略，例如 REINFORCE 算法。为降低方差并提高训练稳定性，后续发展出了 Actor-Critic 方法，其中包括 A2C（Advantage Actor-Critic）和 A3C（Asynchronous Advantage Actor-Critic）。此外，受限策略优化算法（TRPO）和近端策略优化算法（PPO）在实践中也非常受欢迎，PPO 尤其因其实现简单和稳定性好而被广泛使用。
3. **连续动作空间算法**
   针对连续动作问题，常用的算法包括 DDPG（Deep Deterministic Policy Gradient）、TD3（Twin Delayed DDPG）和 SAC（Soft Actor-Critic），这些方法结合了策略梯度和值函数近似，能够高效处理连续动作场景。
4. **新型强化学习变体——GRPO**
   在大语言模型和复杂推理任务的强化学习微调中，出现了一种叫做 GRPO（Group Relative Policy Optimization）的新方法。GRPO可以看作是PPO的一个变体，它放弃了传统需要维护一个与策略模型同等规模的价值网络，而是通过在每个状态下采样一组动作，并计算这些动作的相对奖励（即归一化后的优势）来直接指导策略更新。同时，通过引入 KL 散度约束确保策略更新不过激，从而在计算效率和训练稳定性上都有显著优势。这使得 GRPO 特别适合在大规模语言模型（LLM）如数学推理、代码生成任务中的应用。

综上所述，科研中主流的强化学习算法包括基于值函数的 Q-learning 及其深度版本、基于策略梯度的 REINFORCE 与 Actor-Critic 系列（如 A3C、TRPO、PPO）、以及专为连续动作设计的 DDPG、TD3、SAC；而 GRPO 则是近年来针对大模型微调需求出现的一个有代表性的新方法。
